A graph-based approach for population health analysis using Geo-tagged tweets

Abstract
We propose in this work a graph-based approach for automatic public health analysis using social media. In our approach, graphs are created to model the interactions between features and between tweets in social media. We investigated different graph properties and methods in constructing graph-based representations for population health analysis. The proposed approach is applied in two case studies: (1) estimating health indices, and (2) classifying health situation of counties in the US. We evaluate our approach on a dataset including more than one billion tweets collected in three years 2014, 2015, and 2016, and the health surveys from the Behavioral Risk Factor Surveillance System. We conducted realistic and large-scale experiments on various textual features and graph-based representations. Experimental results verified the robustness of the proposed approach and its superiority over existing ones in both case studies, confirming the potential of graph-based approach for modeling interactions in social networks for population health analysis.

Introduction
Population health measurement reflects the dynamic state of physical, mental, and social well-being of a community [18, 43]. Understanding population health is thus essential for governments to identify health-related concerns and develop strategic healthcare programs for communities.Traditionally, population health data is collected via telephone interviews or postal questionnaires. The benefits of this approach include the simplicity of data collection and the reliability of responses. This is mostly because the questionnaires have been designed by professionals, and the population of interest have been actively and intentionally targeted. Despite these advantages, traditional health surveys exhibit two major limitations: expensive cost and time-consuming. For instance, the budget spent for the Behavioral Risk Factor Surveillance System (BRFSS) survey in Florida, US over 5 years 2011 - 2015 was more than 3.5 million USD,Footnote 1 and the BRFSS reports in 2017 were typically based on the data collected in or before 2015,Footnote 2 which, in turn, could lead to delayed public health policy decisions.Social behaviors of a population provide cues for the health status of that population. The challenge here is how to obtain large-scale and diversified datasets of such behaviors in an automatic and low-cost manner. Fortunately, with the advent of the social networks that allow billions of people to easily connect and communicate, social media has become an abundant and diversified source of information for many healthcare studies [26]. Examples include tracking influenza-like illness in populations from Google search queries [23], localizing illnesses by region [44], or measuring life satisfaction of populations using Twitter data [50]. Importantly, several studies have shown that data collected from social networks (e.g., Twitter) highly correlates to the results achieved by phone-based surveys [19, 35]. Moreover, community behaviors can be analyzed through social media in real-time and at extremely low cost. Therefore, social media, if exploited properly, can provide important insights into understanding people’s health behaviors at both individual and population level. Our work is also motivated by this trend, i.e., exploiting social media as an information source for automatic population health analysis. More specifically, we aim at using geo-tagged tweets to predict and classify population health behaviors and outcomes.Conventional social media-based health analysis methods extract health-related information from the content of the social media data, e.g., from textual features [14, 15] or built via relationships between the features [35, 36]. As shown in many studies in psychology and sociology [5, 53], interactions in social networks are important factors to understanding behaviors of communities [2]. However, this sort of information has not been explored in social media-based healthcare research. In this paper, we propose a graph-based approach taking into account these interactions for population health analysis. Specifically, our contributions are four-fold as follows,


We propose to model interactions in social media data using graph theory. Graphs offer a natural way to capture relationships in data and is often used to represent interactions in social networks. However, existing approaches build graphs from social networks’ users and hence require expensive computation to handle large-scale networks. In contrast, our graphs are constructed in a more manageable and scalable manner. Specifically, we propose two graph structures called inter-feature and inter-tweet graphs built via the coincidence of features and interactions between groups of tweets. To the best of our knowledge, our graph construction methods are novel and our work is the first taking into account interactions in social media data using graphs for population health analysis.


We investigate various graph-based representations defined on the two proposed graph structures for representing social media data at population scale.


We apply our model in two case studies: (1) estimating health indices, and (2) classifying health situation of counties in the US.


We conduct extensive experiments on a large-scale dataset consisting of more than one billion geo-tagged tweets over three years 2014, 2015 and 2016. Experimental results show that our proposed approach outperformed existing ones in both case studies.

Parts of this paper have been published in our recent work [38]. Compared with the previous version, this work makes several extensions. First, in this version, we propose inter-tweet graphs for modelling response behaviour, e.g. like/reply, in social networks. Second, we extend our experiments with the case study of health status classification and provide detailed insights with in-depth discussion.

Related work
Public health analytics with social mediaSocial media has been serving as a rapid vehicle for public health analytics. Social media data has also been proven to be superior to traditional means due to time- and cost-effectiveness [11]. Applications of social media in public healthcare can be found in detection and monitoring of health issues including social well-being, positive mental health, and self-rated health [6], in forecasting public health trends [33], and in developing prevention programs [31].The correlations between social media and clinical datasets haven been demonstrated in various ways [4, 14, 19, 44]. For instance, in [25] Facebook likes were used to predict mortality, diseases, and lifestyle behaviors of 214 counties across the US and the prediction results were shown to be comparable with those obtained from the BRFSS. In [22], tweets were used to build sentiment scores which were found to highly relate to self-rated mental health, sleep quality, and heart disease at census tract level for the city of San Diego over the period of 2014-12-06 to 2017-05-24. In [13], natural language processing tools were applied on social media data, e.g., Twitter, Reddit, and Facebook, to answer public health research questions.Social media offers an effective means for tracking public health attitudes and behaviors. Applications include tracking disease-relevant behaviors and sentiments [49], understanding patient experiences and healthcare quality [48], building disease surveillance systems, supporting public health tracking and prevention [1]. For instance, in [44], health-related tweets with geo-tags were used with the Ailment Topic Aspect Model for tracking influenza over time. The tracking results were benchmarked against the influenza database from the Centers for Disease Control and Prevention (CDC). In [42], health-related tweets were combined with Wikipedia articles for identifying public health concerns in populations. Similarly, in [28], tweets were used to monitor the rate of alcohol consumption across regions in the UK. In [15], linguistic analysis was applied on Twitter’s data and provided a finer-grained representation of population health. In the same manner, tweets have been found useful in prediction of depression in populations [16]. Tweets also have been incorporated with prior knowledge for tracking illnesses, measuring behavioral risk factors, or localizing illnesses by geographic regions [44].Social media also plays an important role in preventive healthcare. For example, launched in 2019, Facebook’s Preventive Health [31] provided preventive health recommendations customized to users’ age and sex. This demonstrates social media’s capacity in encouraging healthy behaviors to a wide population, including vulnerable and isolated ones. Most recently, social media has been integrated to support managing the COVID-19 pandemic in both preparedness and emergency response [32]. As shown in [27], in difficult situations, e.g., during the MERS outbreak in South Korea where the information from public health officials was untrustworthy, social media could be considered as an alternative source.Besides the supportive role, social media has also been found to have detrimental influences on public health [41], e.g., on suicidal behavior [51]. Likewise, social media has been considered to be a main source spreading health misinformation, such as the COVID-19 conspiracy theories [3]. Such information could make people reluctant to engage in health-protective behaviors [3].Social media data typesLiterature has shown the role of various social media data types in population health analysis [37]. Analysis of textual data can provide user demographics, personality, psychological state, and mental health situation. To encode the textual data, textual features are extracted. Those features capture both the content and emotion from the language used in social media [15, 36, 46].In prediction of population health indices through social media, linguistic style is often employed. Linguistic style is an indicator of emotion in the language and has been discovered to highly relate to health outcomes [15, 36]. For instance, when reading depressing stories, judges tend to get depressed accordingly. Based on these findings, a software package, namely Linguistic Inquiry and Word Count (LIWC), was developed to extract psycho-linguistic features from textual data, e.g., documents or tweets [45]. Basically, given a text to be analyzed, the LIWC software first goes through every word of the text, makes comparison between each word with a pre-built dictionary [45], then calculates the percentage of each LIWC category occurring in the given text, and finally results in a list of categories with their rates.Various studies have adopted LIWC features to problems of social media-based health analysis [15, 50]. For instance, Culotta [15] performed linguistic analysis of activities on Twitter to estimate health indices from County Health Rankings and Roadmaps. Experimental results showed significant correlations (with 6 of the 27 indices) between the language that people used and their health situation. This study also indicated that tweets better captured the health status of a community than demographics. In addition, the linguistic style features were found to be predictive of well-being of the US counties [50].Another popular type of textual features in analysis of health-related concerns is latent topics. Topics capture the content of the textual data and can be learned using topic modeling techniques. A commonly used topic modeling method is called latent Dirichlet allocation (LDA) [7]. LDA is an unsupervised technique using Bayesian probabilistic framework to learn latent topics from a corpus. As shown in [50], LDA topics derived from tweets were more useful than LIWC in predicting life satisfaction in the US counties. Moreover, when combined with LIWC, demographic and social-economic controls (age, sex, ethnicity, income, and education), prediction performance was significantly improved.Spatio/temporal-referenced data have also been utilized in public healthcare. In particular, accumulated geo-tagged data can be harnessed to determine health issues, monitor the spread of infectious diseases, and analyze the effects of clinical concepts on public health. For instance, geo-tagged data was employed to estimate geographic densities of clinical concepts in regions of interest [20], cluster groups of data having similar location characteristics [47], and build recommendation systems to advise locations of interest [55]. In [36], temporal information encoded in tweets was augmented with textual features to predict sleep patterns of populations.Graph-based representation of interactionsInteractions among users are an important aspect of social networks and are often modeled as graphs. For instance, Chun et al. [12] constructed a graph to model the interactions in Cyworld social network. In this graph, nodes were users and edges between two users represented communications (writing/responding) between them. Edge weights were computed based on the frequency of communications. Similarly, Leskovec and Horvitz [30] created a communication network for the Microsoft Messenger instant-messaging system in which each user was represented by a node and an edge was formed between nodes if the corresponding users exchanged at least one message during the month of observation. In [53], Wilson et al. argued that, as observed from Facebook data, not all social links represented active social relationships. They then recommended to build interaction graphs with a constraint on the minimum number of interaction events (e.g., respond, like) within a stipulated window of time.The graph models in the existing works, e.g., [30, 53] cannot be applied to our problem for two reasons. First, modeling tweets as nodes and pairwise interactions between tweets as edges is not scalable, especially when dealing with large-scale datasets, e.g., our dataset contains billions of tweets. Second, tweets themselves do not have identities while nodes in a graph require such information.

Proposed approach
As presented in the introductory section, interactions in social media data could be an important implication of behaviors of communities and thus may play a role in predicting health status of populations. In this section, we first describe how to model interactions in social media data using graph theory (Section 3.1). We then introduce graph-based representations of social media data at population scale (Section 3.2). Fig. 1 illustrates the flowchart of our approach.
Fig. 1Flowchart of our approachFull size imageGraph constructionWe define interactions in social media data in two ways: via coincidences of features and responses/likes between groups of tweets. We then model these interactions in so-called inter-feature and inter-tweet graphs.Specifically, let \(T^{P}=\left \{ {t_{1}^{P}}, ... , {t_{N}^{P}}\right \}\) denote a set consisting of N tweets collected from a population P. Suppose that each tweet \({t_{i}^{P}}\in T^{P}\) can be described by a feature vector \(\mathbf {f}^{P}_{i}=\left [f_{i,1}^{P}, ... , f_{i,d}^{P}\right ] \in \mathbb {R}^{d}\), e.g., d = 78 psycho-linguistic features in LIWC.Inter-feature graphHaving the low-level feature vectors \(\left \{ \mathbf {f}^{P}_{1}, ... , \mathbf {f}^{P}_{N}\right \}\), we define the interaction \(I_{j,k}^{P}\), where j, k ∈ {1, ... , d} between two arbitrary features j and k using the radial basis function (RBF) as,
$$ \begin{array}{@{}rcl@{}} I_{j,k}^{P}=\exp\left[-\left( \frac{1}{N}\sum\limits_{i=1}^{N}f_{i,j}^{P}-\frac{1}{N}\sum\limits_{i=1}^{N}f_{i,k}^{P}\right)^{2}/2\sigma^{2}\right] \end{array} $$
                    (1)
                where σ is a free parameter which controls the width of the RBF and is used to normalize feature distances into probabilistic metrics. In our implementation, σ is set to 0.1. We empirically found inter-feature graphs achieved the similar yet best overall performance for σ ∈ [0.1, 1.0]. Note that our features were also normalized into [0, 1].As shown in (1), the representative value for each feature j is accumulated over all the tweets in P. Therefore, by using the RBF, \(I_{j,k}^{P}\) capture the coincidence of features j and k, and thus represent the inter-feature relationships within the population P. The larger \(I_{j,k}^{P}\) is, the more correlated feature j to feature k is.We represent the interactions between features in P via a graph \(G^{P}\left (V^{P},E^{P}\right )\) where \(V^{P}=\left \{ {v_{1}^{P}}, ... , {v_{d}^{P}}\right \} \) is the set of vertices, each vertex corresponds to a feature and EP is the set of undirected edges defined as,
$$ \begin{array}{@{}rcl@{}} E^{P}=\left\{ \left( {v_{j}^{P}},{v_{k}^{P}}\right)\in V^{P}\times V^{P}|I_{j,k}^{P}>\theta\right\} \end{array} $$
                    (2)
                where θ is a user-defined threshold. In our experiment, θ was set so that edges with the top 20% of \(I_{j,k}^{P}\) were maintained in the graph, i.e., only top 20% of highly correlated features were considered. We found graphs whose number of edges take 20-30% of the total number of connections performed best on our dataset.Inter-tweet graphA straightforward approach to model pairwise interactions between tweets is to consider each tweet as a node in a graph and interactions as edges. However, this approach is not scalable. In addition, tweets do not have identities to be nodes. To overcome these issues, we cluster a training tweet set using a K-means algorithm wherein each tweet is encoded by its feature vector and the dissimilarity between two feature vectors is measured by Euclidean distance. This step results in a set of K centroids that are then used in both training and testing. Given a tweet set TP of a population P (TP can be either a training or test set), we cluster TP into K subsets \({T_{1}^{P}}, ... , {T_{K}^{P}}\), i.e., \(T^{P} = \bigcup _{j=1}^{K} {T_{j}^{P}}\). The partition is done by assigning each tweet in TP to its nearest centroid from the K centroids. In our implementation, we empirically set K = 50. We observed that there were subtle changes in the performances of inter-tweet graphs while this setting achieved the best overall performance across all years in our dataset.We define the interaction \(S_{j,k}^{P}\) between two subsets \({T_{j}^{P}}\) and \({T_{k}^{P}}\) as the proportion of the interactions between tweets in these two subsets. Specifically,
$$ \begin{array}{@{}rcl@{}} S_{j,k}^{P}=\frac{{\sum}_{{t_{m}^{P}}\in {T_{j}^{P}}}{\sum}_{{t_{n}^{P}}\in {T_{k}^{P}}}r\left( {t_{m}^{P}},{t_{n}^{P}}\right)}{|{T_{j}^{P}}||{T_{k}^{P}}|} \end{array} $$
                    (3)
                where \(r\left ({t_{m}^{P}},{t_{n}^{P}}\right )=1\) if \({t_{m}^{P}}\) is a response/like to \({t_{n}^{P}}\) or vice versa, and \(r\left ({t_{m}^{P}},{t_{n}^{P}}\right )=0\), otherwise; \(|{T_{j}^{P}}|\) and \(|{T_{k}^{P}}|\) is the cardinality of \({T_{j}^{P}}\) and \({T_{k}^{P}}\) respectively.We then construct a graph \(G^{P}\left (V^{P},E^{P}\right )\) in which each node \({v_{j}^{P}}\in V^{P}\) corresponds to a subset \({T_{j}^{P}}\). Like inter-feature graphs, two nodes \({v_{j}^{P}}\) and \({v_{k}^{P}}\) are connected (by an undirected edge) if their interaction \(S_{j,k}^{P}>\theta \). We note that the subsets \({T_{j}^{P}}\) are deterministic in their feature space and thus they imply identities.Graph-based population representationGiven the population P, a graph \(G^{P}\left (V^{P},E^{P}\right )\) is constructed from either features or tweets as above. The graph-based representation of P is denoted as hP. In the following sub-sections, we present different ways to define hP.Graph propertiesBy using graph properties, hP can be represented as a vector of VP dimensions, i.e., \(\mathbf {h}^{P}=\left [{h_{1}^{P}}, ... , h_{|V^{P}|}^{P}\right ]\) where \({h_{j}^{P}}\) are computed from properties of vertices \({v_{j}^{P}}\). In this work, we investigate commonly used graph properties including Closeness Centrality, Betweenness Centrality, and PageRank [39].
Closeness Centrality (CC)
The closeness centrality of a node \({v_{j}^{P}}\in V^{P}\) is defined as the reciprocal of the sum of the shortest path distances from \({v_{j}^{P}}\) to all other nodes [21],
$$ \begin{array}{@{}rcl@{}} {h_{j}^{P}}=CC\left( {v_{j}^{P}}\right)=\frac{|V^{P}|-1}{{\sum}_{{v_{k}^{P}}\in V^{P}-\left\{ {v_{j}^{P}}\right\} }l\left( {v_{j}^{P}},{v_{k}^{P}}\right)} \end{array} $$
                    (4)
                where \(l\left ({v_{j}^{P}},{v_{k}^{P}}\right )\) is the length of the shortest-path from node \({v_{j}^{P}}\) to node \({v_{k}^{P}}\).
Intuitively, \({h_{j}^{P}}\) represents the proximity of \({v_{j}^{P}}\) to other nodes in the graph GP. For instance, if the graph GP is built based on tweets, \(l\left ({v_{j}^{P}},{v_{k}^{P}}\right )\) represents how often tweets in cluster j interact with tweets in cluster k. If GP is constructed from features, \(l\left ({v_{j}^{P}},{v_{k}^{P}}\right )\) is calculated from the similarity between feature j and feature k. The shorter \(l\left ({v_{j}^{P}},{v_{k}^{P}}\right )\) is, the more direct \({v_{j}^{P}}\) can be linked to \({v_{k}^{P}}\), e.g., more interactions exist between cluster j and cluster k. In other words, \({h_{j}^{P}}\) captures the centrality (or sparsity) of the graph GP.
Betweenness Centrality (BC)
The betweenness centrality of a node \({v_{j}^{P}}\in V^{P}\) is the sum of the fraction of all-pairs shortest paths that pass through \({v_{j}^{P}}\) [10],
$$ \begin{array}{@{}rcl@{}} {h_{j}^{P}}=BC\left( {v_{j}^{P}}\right)=\sum\limits_{{v_{k}^{P}},{v_{l}^{P}}\in V^{P}}\frac{\beta\left( {v_{k}^{P}},{v_{l}^{P}}|{v_{j}^{P}}\right)}{\beta\left( {v_{k}^{P}},{v_{l}^{P}}\right)} \end{array} $$
                    (5)
                where \(\beta \left ({v_{k}^{P}},{v_{l}^{P}}\right )\) is the number of shortest paths from \({v_{k}^{P}}\) to \({v_{l}^{P}}\) and \(\beta \left ({v_{k}^{P}},{v_{l}^{P}}|{v_{j}^{P}}\right )\) is the number of those paths passing through \({v_{j}^{P}}\) other than \({v_{k}^{P}}\) and \({v_{l}^{P}}\). If \({v_{k}^{P}}={v_{l}^{P}}\), \(\beta \left ({v_{k}^{P}},{v_{l}^{P}}\right )=\beta \left ({v_{k}^{P}},{v_{k}^{P}}\right )=1\), and if \({v_{j}^{P}}\in \left \{ {v_{k}^{P}},{v_{l}^{P}}\right \} \), \(\beta \left ({v_{k}^{P}},{v_{l}^{P}}|{v_{j}^{P}}\right )=0\).
As shown in (5), in contrast to closeness centrality, betweeness centrality takes into account indirect connections.
PageRank (PR)
PageRank [40] was developed for measuring the importance of websites on the Internet. This method makes use of an underlying assumption that more important websites are likely to receive more links from others. In our case, we define the PageRank property of a node \({v_{j}^{P}}\in V^{P}\) as:
$$ {h_{j}^{P}}=PR\left( {v_{j}^{P}}\right)=\sum\limits_{{v_{k}^{P}}\in\mathcal{N}\left( {v_{j}^{P}}\right)}\frac{PR\left( {v_{k}^{P}}\right)}{L\left( {v_{k}^{P}}\right)} $$
                    (6)
                where \(\mathcal {N}({v_{j}^{P}})\) is the set of all nodes linking to node \({v_{j}^{P}}\) and \(L({v_{k}^{P}})\) is the number of links from \({v_{k}^{P}}\).
Graph kernelsGraph-based representations hP defined in Section 3.2.1 are created from the graph GP of the population P. Alternatively, one may consider the similarity among different graphs in a training dataset in creating graph-based representations. Graph kernels have been proven an effective tool to calculate the similarity among graph structures [9]. The core idea of graph kernels is to decompose a graph into sub-graphs, then applies a kernel to measure the similarity between these sub-graphs.In general, let P = {Pj} be the set of populations in a training dataset and \(\mathbf {G}=\left \{ G^{P_{j}}\right \}\) be theset of graphs constructed using the methods presented in Section 3.1. The graph-based representation hP of a population P is a vector of |P| dimensions, \(h^{P}=\left [{h_{1}^{P}}, ... , h_{|\mathbf {P}|}^{P}\right ]\) where
$$ {h_{j}^{P}}=\mathcal{K}\left( G^{P},G^{P_{j}}\right)=\langle\phi\left( G^{P}\right),\phi\left( G^{P_{j}}\right)\rangle $$
                    (7)
                where \(G^{P_{j}}\in \mathbf {G}\) and ϕ is a function that maps a graph G into the Hilbert space \({\mathscr{H}}\) that supports the structure of inner products 〈.,.〉.The advantage of using kernel methods is that the mapping function ϕ is not necessary to be determined explicitly. This is because the kernel function \(\mathcal {K}\) can be conveniently computed using inner products. Intuitively, \(\mathcal {K}\) measures the similarity between GP and \(G^{P_{j}}\). Note that if \(G^{P}=G^{P_{j}}\), \(\mathcal {K}\left (G^{P},G^{P_{j}}\right )=1\) (i.e., GPand \(G^{P_{j}}\) are isomorphic).Different kernels make use of different decomposition techniques and similarity measures [9]. For instance, shortest path kernel [8], computing the shortest path lengths between all pairs of nodes in two graphs G and \(G^{\prime }\) is defined as follows,
$$ \mathcal{K}\left( G,G^{\prime}\right)={\Sigma}_{v_{i,}v_{j}\in G}{\Sigma}_{v^{\prime}_{m},v^{\prime}_{n}\in G^{\prime}}\kappa\left( l\left( v_{i},v_{j}\right),l\left( v^{\prime}_{m},v^{\prime}_{n}\right)\right) $$
                    (8)
                where \(l\left (v_{i},v_{j}\right )\) is the length of the shortest path between node vi and vj, and κ is calculated as,
$$ \kappa\left( l\left( v_{i},v_{j}\right),l\left( v^{\prime}_{m},v^{\prime}_{n}\right)\right)=\begin{cases} 1 & \text{if} {l\left( v_{i},v_{j}\right)=l\left( v^{\prime}_{m},v^{\prime}_{n}\right)}\\ 0 & \text{otherwise} \end{cases} $$
                    (9)
                We note that computing the shortest paths between all pairs of nodes in a graph of n nodes can be done efficiently in O(n3) using the Floyd-Warshall algorithm. In this work, we investigate two kernels: shortest path (SP) [8] and Weisfeiler-Lehman (WL) subtree [52].

Experiments
Case studiesWe applied our approach in two case studies: 1) estimating health indices, and 2) classifying health situation of counties in the US.Case study 1. Population health index estimationWe conducted an across-county prediction task to estimate health indices. Technically, this is a regression task where, given a county P, input is a feature vector extracted from that county and output is a population health index. In this case study, three primary health indices in BRFSS: “generic health”, “physical health”, and “mental health” were estimated.We employed a linear regression model for the estimation task. Specifically, the health index yP of a population P can be estimated as follows,
$$ \begin{array}{@{}rcl@{}} y^{P}={\mathbf{w}}^{\top} \mathbf{h}^{P} + e \end{array} $$
                    (10)
                where hP is defined in Section 3.2, \(e\sim N\left (0,\epsilon ^{2}\right )\) is a Gaussian error term, and w is the weight vector that can be learned directly from training data.Case study 2. Population health situation classificationThis case study aims to classify the health status of a given population into two classes: good or bad [54]. Like case study 1, input of each county is the feature vector extracted from that county and output is a health status (good vs bad).We adopted the deep graph convolutional neural network (DGCNN) proposed in [57] for classifying population health status. DGCNN allows end-to-end learning on original graphs without preprocessing while demonstrating state-of-the-art performance on many tasks. To apply DGCNN on our graphs, we used a single network structure consisting of four graph convolutional layers, two 1-D convolutional layers followed by a dense layer, and a softmax layer as output. Activation functions include the hyperbolic tangent function (tanh) in graph convolutional layers and rectified linear units (ReLU) in other layers. Stochastic gradient descent with the Adam updating rule [29] was employed in training the network.DatasetWe crawled 1,129,928,183 tweets in years 2014, 2015, and 2016, with associated US geo-codes. We also collected 152,853,038 tweets made in 2013 for learning latent topics.The collected tweets were associated with the US counties by mapping their geo-codes to the Federal Information Processing System (FIPS) codes using the cartographic boundary files provided by the US Census Bureau in 2013. There were 3,221 different geo-codes (i.e., 3,221 counties) in the US. Note that we used only tweets with associated latitude/longitude coordinates, those with self-reported location information but without coordinates were not considered in our study.We used BRFSS survey reports as the ground truth. The surveys were conducted by the CDC via telephone interviews of the US residents regarding to their health-related risk behaviors, chronic health conditions, and health outcomes. BRFSS contains more than 400,000 interviews conducted each year and is currently the largest health survey system, not only in the US but also in the world. The questionnaires in BRFSS surveys are categorized into core sections including current health status, number of healthy days, inadequate sleep, chronic health conditions, and optional modules such as healthcare access or social context.For estimating health indices (case study 1), we used the annual health ranking data of counties in BRFSS surveys including i) poor or fair health - percent of adults that report fair or poor health, ii) poor physical health days - the average number of reported physically unhealthy days per month, and iii) poor mental health days - the average number of reported mentally unhealthy days per month. The ranges of the health indices in the ground-truth are as follows: [4, 51] for poor or fair health, [1, 10] for poor physical health, and [1, 10] for poor mental health.For classifying health situation (case study 2), for each health index, top 500 counties with highest scores were assigned to “good”, and top 500 counties with lowest scores were assigned to “bad”.Footnote 3Computational resourcesTo process large-scale data in data aggregation, county mapping, feature extraction (from billions of tweets), and graph construction, we employed Spark on top of Hadoop [56]. Spark is a computing platform which enables distributed and parallel computations on a cluster scaled up to 8,000 nodes. Furthermore, Spark is an in-memory based system which is convenient to keep data in memory for subsequent processing, thus allows much faster computations than disk-based systems like Hadoop MapReduce [17]. Specifically, Spark Hadoop cluster comprises 8 CentOS 7.2 physical machines, each of which is equipped with Intel Xeon E5-26700 (8 cores, 16 threads) CPU, 128 GB RAM, Intel Xeon Phi Coprocessor (60 cores), and 24TB HDD.

Results
There are several technical contributions proposed in the paper, including two graph construction methods (Section 3.1), different graph-based representations (Section 3.2), and two textual feature types: LIWC and latent topics. Different combinations of these proposals result in different performances. In the following subsections, we investigate such combinations in each case study and compare our proposed approach with existing works.Case study 1. Population health index estimationWe used 70% of the counties (2,255 counties) in every year for training the regression model in (10) and the remainder (966 counties) for testing it. To measure the performance of health index estimation, we used Spearman’s rank correlation coefficient (or Spearman’s rho [34]): \(rho = 1 - \frac {6{\Sigma }_{i=1}^{n} {d^{2}_{i}}}{n(n^{2}-1)}\) where n is the number of counties and di is the difference between the estimated and actual health index of the i-th county.We first evaluated inter-feature graphs in population heath index estimation. Specifically, we constructed inter-feature graphs using LIWC and latent topics respectively. For LIWC, all 78 features were used. For latent topics, we varied the number of latent topics from 10 to 100. We observed that the performance of health index estimation slightly changed when the number of latent topics was within 50-80 and reached the highest performance when the number of topics was 80. The performance then dropped when the number of latent topics was outside this range. Therefore, 80 topics were used in all following experiments.Table 1 shows the performances in health index estimation of inter-feature graphs built on LIWC and latent topics respectively. For each feature type (LIWC/Topics), we evaluate all graph properties presented in Section 3.2. Experimental results show that, among graph properties, Betweeness Centrality (BC) performed best on both LIWC and latent topics. In addition, WL subtree kernel outperformed SP kernel. Therefore, to make Table 1 easy to follow, only the results of BC property and WL subtree kernel (i.e., the best representations of graph properties and graph kernels) are included. As shown in Table 1, BC slightly but consistently outperforms WL subtree kernel in all cases. In addition, BC achieves the highest performance across all health indices, years, and on both LIWC and latent topics.
Table 1 Case study 1: health index estimation performance (Spearman’s rho) of inter-feature graphs vs existing workFull size tableWe then combined graph-based representations with features used to build the graphs, e.g., BC property + LIWC, and found that these combinations made no improvements on the use of LIWC. In contrast, the combination of BC property and latent topics gained significant advance and also achieved the best overall performance across all years and on all health indices.We compared our graph-based approach with other non-graph-based ones. Culotta in [15] established a seminal in the field of population health analysis through social media. In [15], tweets within the same county were gathered into an aggregated tweet on which features (LIWC and latent topics) were extracted. This approach is referred to as non-graph approach and has been widely adopted in following studies such as [24, 37, 50]. We re-implemented the non-graph approach and evaluated it on our collected data. Comparison results, presented in Table 1, show that graph-based methods, including graph properties and graph kernels, significantly outperform the non-graph ones. In particular, compared with the non-graph approach, the graph-based representations built on inter-feature graphs of latent topics and BC property improved the health index estimation performance on all health indices and in all years, and the improvement was up to 14% in estimation of mental health in year 2014.We evaluated the performances in health index estimation of inter-tweet graphs on LIWC and latent topics and reported results in Table 2. In general, inter-tweet graphs show lower performances in comparison with inter-feature graphs on all health indices, years, and on both LIWC and latent topics. Unlike inter-feature graphs, BC property and WL subtree kernelwith inter-tweet graphs obtained comparable performances. For instance, as shown in Table 2, WL subtree kernel is more dominant than BC property in year 2014 but less favor in years 2015 and 2016. Compared with the non-graph approach, both WL kernel and BC property defined on latent topics showed superior performance. Like inter-feature graphs, the graph-based representations built on latent topics and BC property improved 18% in estimation of mental health in year 2016, in comparison with the non-graph approach.
Table 2 Case study 1: health index estimation performance (Spearman’s rho) of inter-tweet graphs vs existing workFull size tableCase study 2: population health situation classificationAs shown in our experiments, inter-feature graphs significantly outperformed inter-tweet graphs on both LIWC and latent topics. Therefore, in this case study, we focused on inter-feature graphs only. Similarly to case study 1, we also experimented inter-feature graphs on both LIWC and latent topics. For evaluation, we performed 10-fold validation. For each health index, based on the ground truth status scores, the top/bottom 500 counties were considered as true positives (i.e., good) and true negatives (i.e., bad). We show the learning curve of our graph convolution model in Fig. 2.
Fig. 2Learning curves of our graph convolution model on the training and validation set. We used the model trained at 50 epochs in evaluations and comparisonsFull size imageSince this problem is a binary classification problem, we measured the classification performance using,
$$ \begin{array}{@{}rcl@{}} Accuracy &=& \frac{TP+TN}{TP+TN+FP+FN}\\ Sensitivity &=& \frac{TP}{TP + FN}\\ Specificity &=& \frac{TN}{TN + FP} \end{array} $$where TP/TN is the number of cases correctly classified as good/bad and FP/FN is the number of cases incorrectly classified as good/bad.In addition, for each health index, we generated Receiver Operating Characteristic (ROC) curves, representing the trade-off between the sensitivity and specificity, in different training/test splits of the 10-fold setting. We then calculated the mean and standard deviation of Area Under the Curve (AUC) of the ROC curves across all training/test splits.We report the performances of inter-feature graphs built on LIWC and latent topics in case study 2 in Table 3. Experimental results show that, compared with physical and mental health, generic health was always classified at the highest accuracy in all years and on both LIWC and latent topics. The classification accuracy of generic health reached its highest performance in year 2014 at 94% of AUC on latent topics. Physical health took the second place and got its highest position at 91% of AUC on LIWC in 2014. Unlike case study 1, both LIWC and latent topics performed similarly in most cases. Table 3 also shows that the classification was performed consistently across all experimental settings, e.g., the standard deviation of AUC, denoted as “std AUC”, ≤ 6%. We further validate the potential of graph convolution method by illustrating the distributions of features learned by the method in Fig. 3.
Fig. 3Visualization of learning features in graph convolution. Each data point corresponds to a county whose the graph is built on LIWC: a input graph features, b features learned after 50 epochs. Features are created by concatenating node features in graphs and presented in 2D using Principal Component Analysis. The most two prominent components are selected for this visualization. As shown, compared with input features, learned features are better separatedFull size imageTable 3 Case study 2: classification performance of inter-feature graphs on three health indices: generic, physical, and mentalFull size tableLike case study 1, we compared our graph-based approach (i.e., combination of inter-feature graphs and DGCNN) with non-graph ones. For the non-graph methods, e.g., [15], we created features for a population by aggregating features from individual tweets of that population. These features were then fed to a logistic classifier for classifying the population health status. The same evaluation protocol (i.e., 10-fold validation with the same training/test split per fold) was applied. We present the comparison results in Table 4. As shown in our results, our graph-based approach significantly outperforms the non-graph ones (up to 18% on LIWC and 13% on latent topics).
Table 4 Case study 2: comparison of our method with existing ones using mean AUC in three years 2014, 2015, and 2016Full size table

Conclusion
This paper proposes a novel approach for population health analysis through social media. In our approach, interactions in social media data are modeled in graphs and defined via the coincidences of features and responses/likes between groups of tweets. We investigated various graph-based representations. We applied the proposed approach in two tasks: health index estimation and health situation classification of counties in the US, and conducted extensive experiments on a large-scale dataset benchmarked by the Behavioral Risk Factor Surveillance System of the US. Experimental results verified the importance of interactions in social media for health analysis at population scale. Specifically, our approach achieved state-of-the-art performance on both the tasks while inter-feature graphs built on latent topics performed best on health index estimation. Studying the interactions between different feature types and combination of various social media data types, e.g., text, images, etc., will be our future work.
